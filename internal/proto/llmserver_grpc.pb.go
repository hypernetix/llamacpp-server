// Code generated by protoc-gen-go-grpc. DO NOT EDIT.
// versions:
// - protoc-gen-go-grpc v1.3.0
// - protoc             v5.26.1
// source: llmserver.proto

package proto

import (
	context "context"
	grpc "google.golang.org/grpc"
	codes "google.golang.org/grpc/codes"
	status "google.golang.org/grpc/status"
)

// This is a compile-time assertion to ensure that this generated file
// is compatible with the grpc package it is being compiled against.
// Requires gRPC-Go v1.32.0 or later.
const _ = grpc.SupportPackageIsVersion7

const (
	LLMServer_Ping_FullMethodName      = "/proto.LLMServer/Ping"
	LLMServer_LoadModel_FullMethodName = "/proto.LLMServer/LoadModel"
	LLMServer_Predict_FullMethodName   = "/proto.LLMServer/Predict"
)

// LLMServerClient is the client API for LLMServer service.
//
// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://pkg.go.dev/google.golang.org/grpc/?tab=doc#ClientConn.NewStream.
type LLMServerClient interface {
	Ping(ctx context.Context, in *PingRequest, opts ...grpc.CallOption) (*PingResponse, error)
	LoadModel(ctx context.Context, in *LoadModelRequest, opts ...grpc.CallOption) (LLMServer_LoadModelClient, error)
	Predict(ctx context.Context, in *PredictRequest, opts ...grpc.CallOption) (LLMServer_PredictClient, error)
}

type lLMServerClient struct {
	cc grpc.ClientConnInterface
}

func NewLLMServerClient(cc grpc.ClientConnInterface) LLMServerClient {
	return &lLMServerClient{cc}
}

func (c *lLMServerClient) Ping(ctx context.Context, in *PingRequest, opts ...grpc.CallOption) (*PingResponse, error) {
	out := new(PingResponse)
	err := c.cc.Invoke(ctx, LLMServer_Ping_FullMethodName, in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *lLMServerClient) LoadModel(ctx context.Context, in *LoadModelRequest, opts ...grpc.CallOption) (LLMServer_LoadModelClient, error) {
	stream, err := c.cc.NewStream(ctx, &LLMServer_ServiceDesc.Streams[0], LLMServer_LoadModel_FullMethodName, opts...)
	if err != nil {
		return nil, err
	}
	x := &lLMServerLoadModelClient{stream}
	if err := x.ClientStream.SendMsg(in); err != nil {
		return nil, err
	}
	if err := x.ClientStream.CloseSend(); err != nil {
		return nil, err
	}
	return x, nil
}

type LLMServer_LoadModelClient interface {
	Recv() (*LoadModelResponse, error)
	grpc.ClientStream
}

type lLMServerLoadModelClient struct {
	grpc.ClientStream
}

func (x *lLMServerLoadModelClient) Recv() (*LoadModelResponse, error) {
	m := new(LoadModelResponse)
	if err := x.ClientStream.RecvMsg(m); err != nil {
		return nil, err
	}
	return m, nil
}

func (c *lLMServerClient) Predict(ctx context.Context, in *PredictRequest, opts ...grpc.CallOption) (LLMServer_PredictClient, error) {
	stream, err := c.cc.NewStream(ctx, &LLMServer_ServiceDesc.Streams[1], LLMServer_Predict_FullMethodName, opts...)
	if err != nil {
		return nil, err
	}
	x := &lLMServerPredictClient{stream}
	if err := x.ClientStream.SendMsg(in); err != nil {
		return nil, err
	}
	if err := x.ClientStream.CloseSend(); err != nil {
		return nil, err
	}
	return x, nil
}

type LLMServer_PredictClient interface {
	Recv() (*PredictResponse, error)
	grpc.ClientStream
}

type lLMServerPredictClient struct {
	grpc.ClientStream
}

func (x *lLMServerPredictClient) Recv() (*PredictResponse, error) {
	m := new(PredictResponse)
	if err := x.ClientStream.RecvMsg(m); err != nil {
		return nil, err
	}
	return m, nil
}

// LLMServerServer is the server API for LLMServer service.
// All implementations must embed UnimplementedLLMServerServer
// for forward compatibility
type LLMServerServer interface {
	Ping(context.Context, *PingRequest) (*PingResponse, error)
	LoadModel(*LoadModelRequest, LLMServer_LoadModelServer) error
	Predict(*PredictRequest, LLMServer_PredictServer) error
	mustEmbedUnimplementedLLMServerServer()
}

// UnimplementedLLMServerServer must be embedded to have forward compatible implementations.
type UnimplementedLLMServerServer struct {
}

func (UnimplementedLLMServerServer) Ping(context.Context, *PingRequest) (*PingResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method Ping not implemented")
}
func (UnimplementedLLMServerServer) LoadModel(*LoadModelRequest, LLMServer_LoadModelServer) error {
	return status.Errorf(codes.Unimplemented, "method LoadModel not implemented")
}
func (UnimplementedLLMServerServer) Predict(*PredictRequest, LLMServer_PredictServer) error {
	return status.Errorf(codes.Unimplemented, "method Predict not implemented")
}
func (UnimplementedLLMServerServer) mustEmbedUnimplementedLLMServerServer() {}

// UnsafeLLMServerServer may be embedded to opt out of forward compatibility for this service.
// Use of this interface is not recommended, as added methods to LLMServerServer will
// result in compilation errors.
type UnsafeLLMServerServer interface {
	mustEmbedUnimplementedLLMServerServer()
}

func RegisterLLMServerServer(s grpc.ServiceRegistrar, srv LLMServerServer) {
	s.RegisterService(&LLMServer_ServiceDesc, srv)
}

func _LLMServer_Ping_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(PingRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(LLMServerServer).Ping(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: LLMServer_Ping_FullMethodName,
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(LLMServerServer).Ping(ctx, req.(*PingRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _LLMServer_LoadModel_Handler(srv interface{}, stream grpc.ServerStream) error {
	m := new(LoadModelRequest)
	if err := stream.RecvMsg(m); err != nil {
		return err
	}
	return srv.(LLMServerServer).LoadModel(m, &lLMServerLoadModelServer{stream})
}

type LLMServer_LoadModelServer interface {
	Send(*LoadModelResponse) error
	grpc.ServerStream
}

type lLMServerLoadModelServer struct {
	grpc.ServerStream
}

func (x *lLMServerLoadModelServer) Send(m *LoadModelResponse) error {
	return x.ServerStream.SendMsg(m)
}

func _LLMServer_Predict_Handler(srv interface{}, stream grpc.ServerStream) error {
	m := new(PredictRequest)
	if err := stream.RecvMsg(m); err != nil {
		return err
	}
	return srv.(LLMServerServer).Predict(m, &lLMServerPredictServer{stream})
}

type LLMServer_PredictServer interface {
	Send(*PredictResponse) error
	grpc.ServerStream
}

type lLMServerPredictServer struct {
	grpc.ServerStream
}

func (x *lLMServerPredictServer) Send(m *PredictResponse) error {
	return x.ServerStream.SendMsg(m)
}

// LLMServer_ServiceDesc is the grpc.ServiceDesc for LLMServer service.
// It's only intended for direct use with grpc.RegisterService,
// and not to be introspected or modified (even as a copy)
var LLMServer_ServiceDesc = grpc.ServiceDesc{
	ServiceName: "proto.LLMServer",
	HandlerType: (*LLMServerServer)(nil),
	Methods: []grpc.MethodDesc{
		{
			MethodName: "Ping",
			Handler:    _LLMServer_Ping_Handler,
		},
	},
	Streams: []grpc.StreamDesc{
		{
			StreamName:    "LoadModel",
			Handler:       _LLMServer_LoadModel_Handler,
			ServerStreams: true,
		},
		{
			StreamName:    "Predict",
			Handler:       _LLMServer_Predict_Handler,
			ServerStreams: true,
		},
	},
	Metadata: "llmserver.proto",
}
